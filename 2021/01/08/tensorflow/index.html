<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="舒克">





<title>tensorflow | James&#39;s blog</title>



    <link rel="icon" href="/public/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


    


<meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="James's blog" type="application/atom+xml">
</head>
<body>
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">Shuker&#39;s Blog</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>

        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">Shuker&#39;s Blog</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
        <div class="main">
            <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    document.ready(
        function () {
            tocbot.init({
                tocSelector: '.tocbot-list',
                contentSelector: '.post-content',
                headingSelector: 'h1, h2, h3, h4, h5',
                collapseDepth: 1,
                orderedList: false,
                scrollSmooth: true,
            })
        }
    )

    function expand_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 6,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "collapse_toc()");
        b.innerHTML = "Collapse all"
    }

    function collapse_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 1,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "expand_toc()");
        b.innerHTML = "Expand all"
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">tensorflow</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">舒克</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">January 8, 2021&nbsp;&nbsp;17:36:10</a>
                        </span>
                    
                    
                        <span class="post-category">
                    Category:
                            
                                <a href="/categories/Program/">Program</a>
                            
                        </span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <p>简化API，用Keras和eager execution构造模型</p>
<h1 id="tf-keras概述"><a href="#tf-keras概述" class="headerlink" title="tf.keras概述"></a>tf.keras概述</h1><p>tf.kears：允许创建复杂的拓扑，包括使用残差层、自定义多输入输出模型以及强制编写的正向传递</p>
<p>构建和训练模型的核心API</p>
<p>单输入单输出Sequential顺序模型</p>
<p>函数式API</p>
<p>Eager模式与自定义训练</p>
<p>直接迭代和直观调试</p>
<p>tf.GradientTape 求解梯度，自定义训练逻辑</p>
<p>tf.data 加载图片数据与结构化数据</p>
<p>tf.funcion 自动图运算</p>
<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><p>f(x)=ax+b 算出来的f与实际的f的均方差越小越好，使用梯度下降算法优化以选取合适的a和b</p>
<h3 id="tf-kears实现"><a href="#tf-kears实现" class="headerlink" title="tf.kears实现"></a>tf.kears实现</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">model &#x3D; tf.keras.Sequential() #顺序模型</span><br><span class="line">model.add(tf.keras.layers.Dense(1, input_shape&#x3D;(1,))) </span><br><span class="line">#添加层，输出 输入维度是1</span><br><span class="line">model.complie(optimizer&#x3D;&#39;adam&#39;,loss&#x3D;&#39;mse&#39;)</span><br><span class="line">#优化方法是adam，优化的loss函数是均方差函数</span><br><span class="line">history &#x3D; model.fit(x, y, epochs&#x3D;500)</span><br><span class="line">#用history记录训练记录，训练次数为500</span><br><span class="line">model.predict(pd.Series[20]) #输入为20时预测输出</span><br></pre></td></tr></table></figure>

<h2 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h2><p>寻找合适的a和b使 (f-y)^2/n 最小</p>
<p>随机初始化a和b，然后左右移动寻找极值点</p>
<p>输出是一个若干偏导数构成的向量，每个分量对应函数对输入向量的相应分量的偏导</p>
<p>表明每个位置上损失函数增长最快的方向</p>
<h2 id="多层感知器（神经网络）"><a href="#多层感知器（神经网络）" class="headerlink" title="多层感知器（神经网络）"></a>多层感知器（神经网络）</h2><p>根据权重求和达到激活后输出</p>
<p>深度学习：层数更多，使用多层感知器 </p>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p>激活：信号达到 一定强度才输出</p>
<p>relu</p>
<p>sigmoid 一般处理逻辑回归问题（0和1）</p>
<p>tanh</p>
<p>Leak relu</p>
<h3 id="tf-keras实现"><a href="#tf-keras实现" class="headerlink" title="tf.keras实现"></a>tf.keras实现</h3><p>广告在TV radio newspaper上投放后的销量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model = tf.keras.Sequential([</span><br><span class="line">	tf.keras.layers.Dense(<span class="number">10</span>, input_shape=(<span class="number">3</span>,), activation=<span class="string">'relu'</span>)</span><br><span class="line">  <span class="comment">#10个隐藏单元，三维输入，激活函数为relu</span></span><br><span class="line">  tf.keras.layers.Dense(<span class="number">1</span>)</span><br><span class="line">  <span class="comment">#输出为1维</span></span><br><span class="line">])</span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>,loss=<span class="string">'mse'</span>)</span><br><span class="line">model.fit(x, y, epoch=<span class="number">100</span>)</span><br><span class="line">model.predict(data.iloc[:<span class="number">10</span>,<span class="number">-1</span>]) <span class="comment">#前十行</span></span><br></pre></td></tr></table></figure>

<h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><p>概率分布函数：给定输入时输出一个概率值</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>交叉熵损失函数：实际输出与期望输出之间的差距，放大了损失</p>
<p>binary_crossentropy</p>
<p>$H(p,q)=-\Sigma p(x)logq(x)$</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">model &#x3D; tf.keras.Sequential()</span><br><span class="line">model.add(tf.keras.layers.Dense(4, input_shape&#x3D;(15,), activation&#x3D;&#39;relu&#39;))</span><br><span class="line">model.add(tf.keras.layers.Dense(4, activation&#x3D;&#39;relu&#39;))</span><br><span class="line">model.add(tf.keras.layers.Dense(1, activation&#x3D;&#39;sigmoid&#39;))</span><br><span class="line">#添加两层输入层，第二层自动推断输入；输出层使用sigmoid激活</span><br><span class="line">model.compile(optimizer&#x3D;&#39;adam&#39;,</span><br><span class="line">	loss&#x3D;&#39;binary_crossentropy&#39;,</span><br><span class="line">	mentrics&#x3D;[&#39;acc&#39;]</span><br><span class="line">)</span><br><span class="line">#用交叉熵做损失函数，metrics计算正确率</span><br><span class="line">history &#x3D; model.fit(x, y, epochs&#x3D;100)</span><br></pre></td></tr></table></figure>

<h2 id="softmax多分类"><a href="#softmax多分类" class="headerlink" title="softmax多分类"></a>softmax多分类</h2><p>对多个选项的问题（A B C D）</p>
<p>样本分量之和为1</p>
<p>交叉熵：categorical_crossentropy和sparse_categorical_crossentropy</p>
<p>Fashion MNIST数据集：日常物品的灰度图像</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">fashin_mnist &#x3D; tf.keras.datasets.fashion_mnist.load_data()</span><br><span class="line">(train_image, train_lable), (test_image, test_label) &#x3D;  tf.keras.datasets.fashion.load</span><br><span class="line">#加载数据</span><br><span class="line">train_image &#x3D; train_image&#x2F;255 #数据归一化</span><br><span class="line">test_image &#x3D; test_image&#x2F;255</span><br><span class="line">model &#x3D; tf.keras.Sequential()</span><br><span class="line">model.add(tf.keras.layers.Flatten(input_shape&#x3D;(28,28)))</span><br><span class="line"># 28*28的向量</span><br><span class="line">model.add(tf.keras.layers.Dense(128, activation&#x3D;&#39;relu&#39;))</span><br><span class="line">model.add(tf.keras.layers.Dense(10, activation&#x3D;&#39;softmax&#39;))</span><br><span class="line">#变成概率分布输出</span><br><span class="line">model.compile(optimizer&#x3D;&#39;adam&#39;,loss&#x3D;&#39;sparse_categorical_crossentropy&#39;,metrics&#x3D;[&#39;acc&#39;])</span><br><span class="line">#标签为数字时用sparse_categorical_crossentropy</span><br><span class="line">model.fit(train_image, train_lable, epochss&#x3D;5)</span><br><span class="line">model.evaluate(test_image, test_label)</span><br><span class="line">#评价概率</span><br></pre></td></tr></table></figure>

<h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><h2 id="学习速率"><a href="#学习速率" class="headerlink" title="学习速率"></a>学习速率</h2><p>一种超参数（自己设置的参数）或对模型的一种手工可配置的设置</p>
<p>通过查看损失函数值随时间变化曲线判断学习速率的选取</p>
<h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p>高效的计算数据流图中梯度</p>
<p>每层的导数都是后一层的导数与前一层输出之积</p>
<p>先从输入开始5逐一计算每个隐含层 的输出，直到输出层；然后计算导数，从输出层经各隐含层逐一反向传播</p>
<h2 id="优化函数"><a href="#优化函数" class="headerlink" title="优化函数"></a>优化函数</h2><p>必须得两个参数之一</p>
<p>通过名称调用优化器</p>
<p>SGD：随机梯度下降优化器</p>
<p>RMSprop：对学习率进行衰减</p>
<p>Adam：对超参数的选择鲁棒；通过计算梯度的一阶矩估计和二阶矩估计而为不同的参数设计独立的自适应性学习率</p>
<h2 id="网络容量"><a href="#网络容量" class="headerlink" title="网络容量"></a>网络容量</h2><p>神经元越多，层数越多，拟合能力越强，可能产生过拟合</p>
<p>网络容量就是神经元的多少</p>
<h2 id="Dropout抑制过拟合与超参数选择"><a href="#Dropout抑制过拟合与超参数选择" class="headerlink" title="Dropout抑制过拟合与超参数选择"></a>Dropout抑制过拟合与超参数选择</h2><p>过拟合：随着epoch的增加，在训练数据上得分很高，在测试数据上却比价低</p>
<p>欠拟合：训练和测试数据上都得分比较低</p>
<p>dropout：随机丢弃一部分神经元，只激活部分层</p>
<h3 id="如何抑制过拟合"><a href="#如何抑制过拟合" class="headerlink" title="如何抑制过拟合"></a>如何抑制过拟合</h3><p>取平均：用相同的训练数据训练五个不同的神经网络，得到结果取平均值或多数投票</p>
<p>减少神经元之间复杂的共适应关系：权值更新不依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其他特定特征下才有的效果的情况</p>
<p>实际抑制过拟合的操作：增加训练数据；dropout；正则化；调节超参数</p>
<h3 id="添加dropout层"><a href="#添加dropout层" class="headerlink" title="添加dropout层"></a>添加dropout层</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.add(tf.keras.layers.Dropout(0.5))</span><br><span class="line">#只激活50%</span><br></pre></td></tr></table></figure>

<h1 id="函数式API"><a href="#函数式API" class="headerlink" title="函数式API"></a>函数式API</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">input &#x3D; keras.Input(shape&#x3D;(28,28))</span><br><span class="line">x &#x3D; keras.layers.Flatten()(input)</span><br><span class="line">#传入input调用Flatten层这个函数</span><br><span class="line">x &#x3D; keras.layers.Dense(32, activation&#x3D;&#39;relu&#39;)(x)</span><br><span class="line">#输出32个隐藏单元数</span><br><span class="line">x &#x3D; keras.layers.Dropout(0.5)(x)</span><br><span class="line">x &#x3D; keras.layers.Dense(64, activation&#x3D;&#39;relu&#39;)(x)</span><br><span class="line">output &#x3D; keras.layers.Dense(10, activation&#x3D;&#39;softmax&#39;)(x)</span><br><span class="line">model &#x3D; keras.Model(inputs&#x3D;input, outputs&#x3D;output)</span><br><span class="line">#构建模型</span><br><span class="line">model.compile(optimizer&#x3D;&#39;adam&#39;,</span><br><span class="line">							loss&#x3D;&#39;sparse_categorical_crossentropy&#39;,</span><br><span class="line">							metrics&#x3D;[&#39;accuracy&#39;])</span><br><span class="line">#编译模型</span><br></pre></td></tr></table></figure>

<p>判断两个图片相似度</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x1 &#x3D; keras.layers.Flatten()(input1)</span><br><span class="line">x2 &#x3D; keras.layers.Flatten()(input2)</span><br><span class="line">x &#x3D; keras.layers.concatenate([x1,x2])</span><br><span class="line">x &#x3D; keras.layers.Dense(32, activation&#x3D;&#39;relu&#39;)(x)</span><br><span class="line">output &#x3D; keras.layers.Dense(1, activation&#x3D;&#39;sigmoid&#39;)(x)</span><br></pre></td></tr></table></figure>

<h1 id="Tensorflow2-0基础"><a href="#Tensorflow2-0基础" class="headerlink" title="Tensorflow2.0基础"></a>Tensorflow2.0基础</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">node1 &#x3D; tf.constant([3.0,1.5], [2.5,6.0],tf.float32)</span><br><span class="line">#常量2*2的二维数组</span><br><span class="line">node2 &#x3D; tf.constant([4.0,1.0], [5.0,2.5],tf.float32)</span><br><span class="line">node3 &#x3D; tf.add(node1,node2)#加法</span><br><span class="line">shape#数据形状</span><br><span class="line">dtype#数据类型</span><br><span class="line">print(node3.numpy())#输出运算结果Tensor的值</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tens1 &#x3D; tf.constant([1,2,3])</span><br><span class="line">sess &#x3D; tf.Session()</span><br><span class="line">#创建会话得到运算结果</span><br><span class="line">sess.run(tens1)</span><br><span class="line">sess.close()</span><br><span class="line">#发生异常也可以关闭窗口</span><br><span class="line">with sess.as_default():</span><br><span class="line">	print(result.eval())</span><br><span class="line">#print(result.eval(session&#x3D;sess)) 执行session</span><br></pre></td></tr></table></figure>

<h1 id="卷积CNN"><a href="#卷积CNN" class="headerlink" title="卷积CNN"></a>卷积CNN</h1><h2 id="相比于全连接网络"><a href="#相比于全连接网络" class="headerlink" title="相比于全连接网络"></a>相比于全连接网络</h2><p>对于600x600x3的图像，彩色RGB为3，会产生过拟合</p>
<p>训练速度过慢</p>
<h2 id="各层结构"><a href="#各层结构" class="headerlink" title="各层结构"></a>各层结构</h2><p>输入层：每个像素代表一个特征节点输入到网络</p>
<p>卷积层：使原信号特征增强，降低噪音 </p>
<p>降采样层：降低网络训练参数及模型的过拟合程度</p>
<p>（池化层）：降维</p>
<p>全连接层：对生成的特征进行加权</p>
<p>Softmax层：获得当前样例属于不同类别的概率</p>
<h2 id="卷积运算"><a href="#卷积运算" class="headerlink" title="卷积运算"></a>卷积运算</h2><p>卷积：在2维输入数据上“滑动”计算得到特征图</p>
<p>滑动：对于5x5的图像，选取3x3的9个点加权求和（利用卷积核），填入另一个25x9的表格中；在计算一个3x3的数据后向右移动一格，继续计算</p>
<p>步长：滑动的格数</p>
<p>卷积核：权值矩阵</p>
<p>局部连接：每个输入特征不用查看每个输入特征，只查看部分</p>
<p>权值共享：卷积核在图像上滑动过程中保持不变</p>
<p>填充：用像素为0的值填充边缘，使提取后图像仍然是5x5大小</p>
<p>池化：计算某特定特征的平均值（取背景信息）或最大值（纹理化）；在语义上把相似的特征合并起来</p>
<h2 id="卷积操作函数"><a href="#卷积操作函数" class="headerlink" title="卷积操作函数"></a>卷积操作函数</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from PIL import Image</span><br><span class="line"></span><br><span class="line">def ImgConvolve(image_array,kernel):</span><br></pre></td></tr></table></figure>

<pre><code>image_array：图像矩阵
kernel：卷积核
返回：原图像与算子进行卷积后的结果
```
image_arr = image_array.copy()
img_dim1,img_dim2 = image_arr.shape
k_dim1,k_dim2 = kernel.shape
AddW = int((k_dim1-1)/2)
AddH = int((k_dim2-1)/2)

temp = np.zeros([img_dim1+AddW*2,img_dim2+AddH*2])
#padding填充
temp[AddW:AddW+img_dim1,AddH:AddH+img_dim2] = image_arr[:,:]
#拷贝原图到临时图片中央
output = np.zeros_like(a=temp)
for i in range(AddW,AddW+img_dim1):
    for j in range(AddH,AddH+img_dim2):
        output[i][j] = int(np.sum(temp[i-AddW:i+AddW+1,j-        AddW:j+AddW+1]*kernel))
        #图线和权值点乘
return output[AddW:AddW+img_dim1,AddH:AddH+img_dim2]</code></pre>
        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>舒克</span>
                    </p>
                
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2020 <a href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/Python/"># Python</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2021/02/11/Numpy/">Numpy</a>
            
            
            <a class="next" rel="next" href="/2020/11/13/anaconda/">anaconda</a>
            
        </section>


    </article>
</div>

        </div>
        <footer id="footer" class="footer">
    <div class="copyright">
        <span>© 舒克 | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>

    </div>
</body>
</html>
